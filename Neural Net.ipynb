{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Coding a 2 layer Neural Network from scratch\" and will go through the essentials in making a neural network \n",
    "\n",
    "This is with the intention of utilising this base code in my third year individual project. \n",
    "https://towardsdatascience.com/coding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the neural net\n",
    "\n",
    "This requires all the spaces in memory which will store values as the net makes a pass going forward, and which it will need to call again later. \n",
    "There is still some functionality and python linguistics which I do not understand, however, this is helpful in understanding the overall concepts of the neural net. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "        \n",
    "def reLu(Z):   \n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "\n",
    "\n",
    "#now its time for the back propogation\n",
    "#this includes finding the differentials for all the functions, so that you can see in which way the gradients \n",
    "#are travelling, and therefore look for the weights which will minimize the loss function.\n",
    "def derivativeRelu(x):\n",
    "    x[x<0] = 0 \n",
    "    x[x>0] = 1\n",
    "    return x \n",
    "\n",
    "def derivativeSigmoid(Z):\n",
    "    s = 1/(1 + np.exp(-Z))\n",
    "    dz = s * ( 1-s)\n",
    "    return dz\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class myNet: \n",
    "    \n",
    "    #This will set up the neural net, with the necessary dictionaries, arrays and matrices which will store the input\n",
    "    #and other necessary values \n",
    "    def __init__(self, inputVal, results):\n",
    "        self.X = inputVal\n",
    "        self.Y = results\n",
    "        self.Yh = np.zeros((1, self.Y.shape[1]))\n",
    "        self.threshold = 0.5\n",
    "        \n",
    "        self.numLayers = 2\n",
    "        self.dimensions = [9, 15, 1]\n",
    "        \n",
    "    \n",
    "        #these dictionaries are for storing necessary data\n",
    "        self.param = {}\n",
    "        self.cache = {}\n",
    "        self.gradients = {}\n",
    "        \n",
    "        self.loss = []\n",
    "        self.learningRate = 0.003\n",
    "        self.trainingSamples = self.Y.shape[1]\n",
    "        \n",
    "        \n",
    "        \n",
    "    #This function will set up the initial weights and biases randomly \n",
    "    def paramsInit(self):\n",
    "        np.random.seed(1)\n",
    "        self.param['W1'] = np.random.randn(self.dimensions[1], self.dimensions[0]) / np.sqrt(self.dimensions[0])\n",
    "        self.param['b1'] = np.zeros((self.dimensions[1], 1))\n",
    "        \n",
    "        self.param['W2'] = np.random.randn(self.dimensions[2], self.dimensions[1]) / np.sqrt(self.dimensions[1])\n",
    "        self.param['b2'] = np.zeros((self.dimensions[2], 1))\n",
    "    \n",
    "        return\n",
    "    \n",
    "    \n",
    "    #Here are the functions for the forward pass and initial calculations\n",
    "    \n",
    "    # The activation functions we will be using in this forward pass\n",
    "    # the activation functions are necessary to ensure that the whole neural net does not remain linear \n",
    "    # if it remains linear, the net will not produce more accurate results. \n",
    "    \n",
    "\n",
    "    def forwardPass(self):\n",
    "        Z1 = self.param['W1'].dot(self.X) + self.param['b1']\n",
    "        A1 = reLu(Z1)\n",
    "        self.cache['Z1'], self.cache['A1'] = Z1, A1 \n",
    "        \n",
    "        Z2 = self.param['W2'].dot(A1) + self.param['b2']\n",
    "        A2 = sigmoid(Z2)\n",
    "        self.cache['Z2'], self.cache['A2'] = Z2, A2 \n",
    "        \n",
    "        self.Yh = A2\n",
    "        loss = self.currentClassificationLoss(A2)\n",
    "        \n",
    "        return self.Yh, loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    #This neural net will be doing binary classification and so is using the loss function called Cross-Entropy\n",
    "    #loss function\n",
    "    def currentClassificationLoss(self, Yh):\n",
    "        loss = (1./self.trainingSamples) * (-np.dot(self.Y, np.log(Yh).T) - np.dot(1-self.Y, np.log(1-Yh).T))\n",
    "        return loss \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    #need to find the derivatives for Loss with respect to W1 W2 B1 B2, this is done through calculus and \n",
    "    #backpropogation - THE GRADIENT DESCENT OPTIMIZATION ALGORITHM \n",
    "\n",
    "    def backwards(self):\n",
    "        \n",
    "        derivativeLossYh = - ( np.divide(self.Y, self.Yh) - np.divide(1 - self.Y, 1 - self.Yh))\n",
    "        derivativeLossZ2 = derivativeLossYh * derivativeSigmoid(self.cache['Z2'])\n",
    "        derivativeLossA1 = np.dot(self.param[\"W2\"].T, derivativeLossZ2)\n",
    "        derivativeLossW2 = 1./self.cache['A1'].shape[1] * np.dot(derivativeLossZ2, self.cache['A1'].T)\n",
    "        derivativeLossB2 = 1./self.cache['A1'].shape[1] * np.dot(derivativeLossZ2, np.ones([derivativeLossZ2.shape[1],1]))\n",
    "        \n",
    "        derivativeLossZ1 = derivativeLossA1 * derivativeRelu(self.cache['Z1'])\n",
    "        derivativeLossA0 = np.dot(self.param[\"W1\"].T, derivativeLossZ1)\n",
    "        derivativeLossW1 = 1./self.X.shape[1] * np.dot(derivativeLossZ1 ,self.X.T)\n",
    "        derivativeLossB1 = 1./self.X.shape[1] * np.dot(derivativeLossZ1, np.ones([derivativeLossZ1.shape[1],1])) \n",
    "        \n",
    "        \n",
    "        #if the derivative is negative, it means increasing the weight makes the loss decrease\n",
    "        #if the derivative is positive, it means decreasing the weight makes the loss decrease \n",
    "        self.param[\"W1\"] = self.param[\"W1\"] - self.learningRate * derivativeLossW1\n",
    "        self.param[\"b1\"] = self.param[\"b1\"] - self.learningRate * derivativeLossB1\n",
    "        self.param[\"W2\"] = self.param[\"W2\"] - self.learningRate * derivativeLossW2\n",
    "        self.param[\"b2\"] = self.param[\"b2\"] - self.learningRate * derivativeLossB2\n",
    "        \n",
    "\n",
    "    \n",
    "# all this does is run through the neural net several times (iterations declared) and it will look for the minima in \n",
    "# the region which will reduce the loss. \n",
    "    def gd(self,X, Y, iter = 3000):\n",
    "        np.random.seed(1)                         \n",
    "    \n",
    "        self.paramsInit()\n",
    "    \n",
    "        for i in range(0, iter):\n",
    "            Yh, loss=self.forwardPass()\n",
    "            self.backwards()\n",
    "        \n",
    "            if i % 500 == 0:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, loss))\n",
    "                self.loss.append(loss)\n",
    "                plt.plot(np.squeeze(self.loss))\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Iter')\n",
    "        plt.title(\"Lr =\" + str(self.learningRate))\n",
    "        plt.show()\n",
    "        return\n",
    "    \n",
    "    def pred(self,x, y):  \n",
    "        self.X=x\n",
    "        self.Y=y\n",
    "        comp = np.zeros((1,x.shape[1]))\n",
    "        pred, loss= self.forwardPass()    \n",
    "\n",
    "        for i in range(0, pred.shape[1]):\n",
    "            if pred[0,i] > self.threshold: comp[0,i] = 1\n",
    "            else: comp[0,i] = 0\n",
    "\n",
    "        print(\"Acc: \" + str(np.sum((comp == y)/x.shape[1])))\n",
    "\n",
    "        return comp\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "    Now we have completed the forward pass, made a prediction, calculated the loss, and updated the parameters \n",
    "    to improve the weightings so they make a better prediction. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    After this point the most important work is improving the hyperparameters and improving the input data (feature engineering) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data analysis and running it on the neural net \n",
    "\n",
    "a few changes will need to be made to the data:\n",
    "- firstly, you need to clean the data, removing any missing data points \n",
    "- secondly, neural nets work better in the range 0 - 1 for a prediction (that is why we are using sigmoids function), therefore we will ensure that the data produces a value between 0 and 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002945</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1015425</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016277</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0   1   2   3   4   5   6   7   8   9   10\n",
       "0  1000025   5   1   1   1   2   1   3   1   1   2\n",
       "1  1002945   5   4   4   5   7  10   3   2   1   2\n",
       "2  1015425   3   1   1   1   2   2   3   1   1   2\n",
       "3  1016277   6   8   8   1   3   4   3   7   1   2\n",
       "4  1017023   4   1   1   3   2   1   3   1   1   2"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/danielraad/Desktop/layeredneuralnet/breast-cancer-wisconsin.csv', header=None)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,10].replace(2, 0,inplace=True)\n",
    "df.iloc[:,10].replace(4, 1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing missing rows \n",
    "df = df[df[6] != '?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002945.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1015425.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016277.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017023.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1017122.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1018099.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1018561.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1033078.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1033078.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1035283.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1036172.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1041801.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1043999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1044572.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0    1     2     3     4    5     6    7    8    9    10\n",
       "0   1000025.0  5.0   1.0   1.0   1.0  2.0   1.0  3.0  1.0  1.0  0.0\n",
       "1   1002945.0  5.0   4.0   4.0   5.0  7.0  10.0  3.0  2.0  1.0  0.0\n",
       "2   1015425.0  3.0   1.0   1.0   1.0  2.0   2.0  3.0  1.0  1.0  0.0\n",
       "3   1016277.0  6.0   8.0   8.0   1.0  3.0   4.0  3.0  7.0  1.0  0.0\n",
       "4   1017023.0  4.0   1.0   1.0   3.0  2.0   1.0  3.0  1.0  1.0  0.0\n",
       "5   1017122.0  8.0  10.0  10.0   8.0  7.0  10.0  9.0  7.0  1.0  1.0\n",
       "6   1018099.0  1.0   1.0   1.0   1.0  2.0  10.0  3.0  1.0  1.0  0.0\n",
       "7   1018561.0  2.0   1.0   2.0   1.0  2.0   1.0  3.0  1.0  1.0  0.0\n",
       "8   1033078.0  2.0   1.0   1.0   1.0  2.0   1.0  1.0  1.0  5.0  0.0\n",
       "9   1033078.0  4.0   2.0   1.0   1.0  2.0   1.0  2.0  1.0  1.0  0.0\n",
       "10  1035283.0  1.0   1.0   1.0   1.0  1.0   1.0  3.0  1.0  1.0  0.0\n",
       "11  1036172.0  2.0   1.0   1.0   1.0  2.0   1.0  2.0  1.0  1.0  0.0\n",
       "12  1041801.0  5.0   3.0   3.0   3.0  2.0   3.0  4.0  4.0  1.0  1.0\n",
       "13  1043999.0  1.0   1.0   1.0   1.0  2.0   3.0  3.0  1.0  1.0  0.0\n",
       "14  1044572.0  8.0   7.0   5.0  10.0  7.0   9.0  5.0  5.0  4.0  1.0"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data normalization to ensure the values are between 0 and 1\n",
    "# various ways to perform data normalization, it depends on the data you are using and what your goals are.\n",
    "# two popular techniques are min max scaling and standerdising \n",
    "# here, min max scaling is used \n",
    "# both min max and the standerdizer are susceptible to outliers and so there are others you can look at in the future\n",
    "\n",
    "names = df.columns[0:10]\n",
    "scaler = MinMaxScaler() \n",
    "scaled_df = scaler.fit_transform(df.iloc[:,0:10]) \n",
    "scaled_df = pd.DataFrame(scaled_df, columns=names)\n",
    "scaled_df[10] = df[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.069946</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.070164</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.071096</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.071160</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.071216</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.071223</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.071296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.071331</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.072415</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.072415</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.072579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.072646</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.073066</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.073230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.073273</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.069946  0.444444  0.000000  0.000000  0.000000  0.111111  0.000000   \n",
       "1   0.070164  0.444444  0.333333  0.333333  0.444444  0.666667  1.000000   \n",
       "2   0.071096  0.222222  0.000000  0.000000  0.000000  0.111111  0.111111   \n",
       "3   0.071160  0.555556  0.777778  0.777778  0.000000  0.222222  0.333333   \n",
       "4   0.071216  0.333333  0.000000  0.000000  0.222222  0.111111  0.000000   \n",
       "5   0.071223  0.777778  1.000000  1.000000  0.777778  0.666667  1.000000   \n",
       "6   0.071296  0.000000  0.000000  0.000000  0.000000  0.111111  1.000000   \n",
       "7   0.071331  0.111111  0.000000  0.111111  0.000000  0.111111  0.000000   \n",
       "8   0.072415  0.111111  0.000000  0.000000  0.000000  0.111111  0.000000   \n",
       "9   0.072415  0.333333  0.111111  0.000000  0.000000  0.111111  0.000000   \n",
       "10  0.072579  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "11  0.072646  0.111111  0.000000  0.000000  0.000000  0.111111  0.000000   \n",
       "12  0.073066  0.444444  0.222222  0.222222  0.222222  0.111111  0.222222   \n",
       "13  0.073230  0.000000  0.000000  0.000000  0.000000  0.111111  0.222222   \n",
       "14  0.073273  0.777778  0.666667  0.444444  1.000000  0.666667  0.888889   \n",
       "\n",
       "          7         8         9    10  \n",
       "0   0.222222  0.000000  0.000000  0.0  \n",
       "1   0.222222  0.111111  0.000000  0.0  \n",
       "2   0.222222  0.000000  0.000000  0.0  \n",
       "3   0.222222  0.666667  0.000000  0.0  \n",
       "4   0.222222  0.000000  0.000000  0.0  \n",
       "5   0.888889  0.666667  0.000000  1.0  \n",
       "6   0.222222  0.000000  0.000000  0.0  \n",
       "7   0.222222  0.000000  0.000000  0.0  \n",
       "8   0.000000  0.000000  0.444444  0.0  \n",
       "9   0.111111  0.000000  0.000000  0.0  \n",
       "10  0.222222  0.000000  0.000000  0.0  \n",
       "11  0.111111  0.000000  0.000000  0.0  \n",
       "12  0.333333  0.333333  0.000000  1.0  \n",
       "13  0.222222  0.000000  0.000000  0.0  \n",
       "14  0.444444  0.444444  0.333333  1.0  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is ready to be used. The next step is to train the model on the data. \n",
    "We need to be aware of both overfitting and underfitting, overfitting being the bigger issue. \n",
    "This requires regularization which comes in many ways. I will add regularization following the completion of the tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=scaled_df.iloc[0:500,1:10].values.transpose()\n",
    "y=df.iloc[0:500,10:].values.transpose()\n",
    "xval=scaled_df.iloc[501:683,1:10].values.transpose()\n",
    "yval=df.iloc[501:683,10:].values.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.673967\n",
      "Cost after iteration 500: 0.388928\n",
      "Cost after iteration 1000: 0.231340\n",
      "Cost after iteration 1500: 0.171447\n",
      "Cost after iteration 2000: 0.146433\n",
      "Cost after iteration 2500: 0.133993\n",
      "Cost after iteration 3000: 0.126808\n",
      "Cost after iteration 3500: 0.122107\n",
      "Cost after iteration 4000: 0.118650\n",
      "Cost after iteration 4500: 0.116027\n",
      "Cost after iteration 5000: 0.113934\n",
      "Cost after iteration 5500: 0.112221\n",
      "Cost after iteration 6000: 0.110783\n",
      "Cost after iteration 6500: 0.109546\n",
      "Cost after iteration 7000: 0.108474\n",
      "Cost after iteration 7500: 0.107537\n",
      "Cost after iteration 8000: 0.106710\n",
      "Cost after iteration 8500: 0.105977\n",
      "Cost after iteration 9000: 0.105317\n",
      "Cost after iteration 9500: 0.104718\n",
      "Cost after iteration 10000: 0.104174\n",
      "Cost after iteration 10500: 0.103674\n",
      "Cost after iteration 11000: 0.103216\n",
      "Cost after iteration 11500: 0.102785\n",
      "Cost after iteration 12000: 0.102377\n",
      "Cost after iteration 12500: 0.101980\n",
      "Cost after iteration 13000: 0.101604\n",
      "Cost after iteration 13500: 0.101251\n",
      "Cost after iteration 14000: 0.100912\n",
      "Cost after iteration 14500: 0.100592\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn = myNet(x,y)\n",
    "nn.learningRate=0.01\n",
    "nn.dimensions = [9, 15, 1]\n",
    "nn.gd(x, y, iter = 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCf(a,b,t):\n",
    "    cf =confusion_matrix(a,b)\n",
    "    plt.imshow(cf,cmap=plt.cm.Blues,interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.title(t)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    tick_marks = np.arange(len(set(a))) # length of classes\n",
    "    class_labels = ['0','1']\n",
    "    plt.xticks(tick_marks,class_labels)\n",
    "    plt.yticks(tick_marks,class_labels)\n",
    "    thresh = cf.max() / 2.\n",
    "    for i,j in itertools.product(range(cf.shape[0]),range(cf.shape[1])):\n",
    "        plt.text(j,i,format(cf[i,j],'d'),horizontalalignment='center',color='white' if cf[i,j] >thresh else 'black')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.9620000000000003\n",
      "Acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "pred_train = nn.pred(x, y)\n",
    "pred_test = nn.pred(xval, yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
