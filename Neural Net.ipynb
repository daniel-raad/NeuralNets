{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Coding a 2 layer Neural Network from scratch\" and will go through the essentials in making a neural network \n",
    "\n",
    "This is with the intention of utilising this base code in my third year individual project. \n",
    "https://towardsdatascience.com/coding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the neural net\n",
    "\n",
    "This requires all the spaces in memory which will store values as the net makes a pass going forward, and which it will need to call again later. \n",
    "There is still some functionality and python linguistics which I do not understand, however, this is helpful in understanding the overall concepts of the neural net. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNet: \n",
    "    \n",
    "    #This will set up the neural net, with the necessary dictionaries, arrays and matrices which will store the input\n",
    "    #and other necessary values \n",
    "    def init(self, inputVal, results):\n",
    "        self.X = inputVal\n",
    "        self.Y = results\n",
    "        self.Yh = np.zeros((1, self.Y.shape[1]))\n",
    "        \n",
    "        \n",
    "        self.numLayers = 2\n",
    "        self.dimensions = [9, 15, 1]\n",
    "        \n",
    "    \n",
    "        #these dictionaries are for storing necessary data\n",
    "        self.param = {}\n",
    "        self.cache = {}\n",
    "        self.gradients = {}\n",
    "        \n",
    "        self.loss = []\n",
    "        self.learningRate = 0.003\n",
    "        self.trainingSamples = self.Y.shape[1]\n",
    "        \n",
    "        \n",
    "        \n",
    "    #This function will set up the initial weights and biases randomly \n",
    "    def paramsInit(self):\n",
    "        np.randomSeed(1)\n",
    "        self.param['W1'] = np.random.randn(self.dimensions[1], self.dimensions[0]) / np.sqrt(self.dimensions[0])\n",
    "        self.param['b1'] = np.zeros(self.dimensions[1], 1)\n",
    "        \n",
    "        self.param['W2'] = np.random.randn(self.dimensions[2], self.dimensions[1]) / np.sqrt(self.dimensions[1])\n",
    "        self.param['b2'] = np.zeros(self.dimensions[2], 1)\n",
    "    \n",
    "        return\n",
    "    \n",
    "    \n",
    "    #Here are the functions for the forward pass and initial calculations\n",
    "    \n",
    "    #The activation functions we will be using in this forward pass \n",
    "    def sigmoid(Z):\n",
    "        return 1/(1+np.exp(-Z))\n",
    "        \n",
    "    def reLu(Z):\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def forwardPass(self):\n",
    "        Z1 = self.param['W1'].dot(self.X) + self.param['b1']\n",
    "        A1 = reLu(Z1)\n",
    "        self.cache['Z1'], self.cache['A1'] = Z1, A1 \n",
    "        \n",
    "        Z2 = self.param['W2'].dot(A1) + self.param['b2']\n",
    "        A2 = sigmoid(Z2)\n",
    "        self.cache['Z2'], self.cache['A2'] = Z2, A2 \n",
    "        \n",
    "        self.Yh = A2\n",
    "        loss = self.currentClassificationLoss(A2)\n",
    "        \n",
    "        return self.Yh, loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    #This neural net will be doing binary classification and so is using the loss function called Cross-Entropy\n",
    "    #loss function\n",
    "    def currentClassificationLoss(self, Yh):\n",
    "        loss = (1./self.trainingSamples) * (-np.dot(self.Y, np.log(Yh).T) - np.dot(1-self.Y, np.log(1-Yh).T))\n",
    "        return loss \n",
    "    \n",
    "    \n",
    "    \n",
    "    #now its time for the back propogation\n",
    "    #this includes finding the differentials for all the functions, so that you can see in which way the gradients \n",
    "    #are travelling \n",
    "    \n",
    "    \n",
    "    def derivativeRelu(x):\n",
    "        x[x<0] = 0 \n",
    "        x[x>0] = 1\n",
    "        return x \n",
    "    \n",
    "    def derivativeSigmoid(Z):\n",
    "        s = 1/(1 + np.exp(-Z))\n",
    "        dz = s * ( 1-s)\n",
    "        return dz\n",
    "    \n",
    "    \n",
    "    #need to find the derivatives for Loss with respect to W1 W2 B1 B2, this is done through calculus and \n",
    "    #backpropogation - THE GRADIENT DESCENT OPTIMIZATION ALGORITHM \n",
    "\n",
    "    def backwards(self):\n",
    "        \n",
    "        derivativeLossYh = - ( np.divide(self.Y, self.Yh) - np.divide(1 - self.Y, 1 - self.Yh))\n",
    "        derivativeLossZ2 = derivativeLossYh * derivativeSigmoid(self.cache['Z2'])\n",
    "        derivativeLossA1 = np.dot(self.param[\"W2\"].T, derivativerLossZ2)\n",
    "        derivativeLossW2 = 1./self.cache['A1'].shape[1] * np.dot(derivativeLossZ2, self.cache['A1'].T)\n",
    "        derivativeLossB2 = 1./self.cache['A1'].shape[1] * np.dot(derivativeLossZ2, np.ones([derivativeLossZ1.shape[1],1]))\n",
    "        \n",
    "        derivativeLossZ1 = derivativeLossA1 * derivateRelu(self.cache['Z1'])\n",
    "        derivativeLossA0 = np.dot(self.param[\"W1\"].T, derivativeLossZ2)\n",
    "        derivativeLossW1 = 1./self.X.shape[1] * np.dot(derviativeLossZ1 ,self.X.T)\n",
    "        derivativeLossB1 = 1./self.X.shape[1] * np.dot(derivativeLossZ1, np.ones([derivativeLossZ1.shape[1],1])) \n",
    "        \n",
    "        \n",
    "        #if the derivative is negative, it means increasing the weight makes the loss decrease\n",
    "        #if the derivative is positive, it means decreasing the weight makes the loss decrease \n",
    "        self.param[\"W1\"] = self.param[\"W1\"] - self.learningRate * derivativeLossW1\n",
    "        self.param[\"b1\"] = self.param[\"b1\"] - self.learningRate * derivativeLossB1\n",
    "        self.param[\"W2\"] = self.param[\"W2\"] - self.learningRate * derivativeLossW2\n",
    "        self.param[\"b2\"] = self.param[\"b2\"] - self.learningRate * derivativeLossB2\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "    Now we have completed the forward pass, made a prediction, calculated the loss, and updated the parameters \n",
    "    to improve the weightings so they make a better prediction. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = myNet(x, y)\n",
    "nn.gd(x, y, iter = 15000)\n",
    "\n",
    "\n",
    "# all this does is run through the neural net several times (iterations declared) and it will look for the minima in \n",
    "# the region which will reduce the loss. \n",
    "def gd(self,X, Y, iter = 3000):\n",
    "        np.random.seed(1)                         \n",
    "    \n",
    "        self.paramsInit()\n",
    "    \n",
    "        for i in range(0, iter):\n",
    "            Yh, loss=self.forwardPass()\n",
    "            self.backwards()\n",
    "        \n",
    "            if i % 500 == 0:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, loss))\n",
    "                self.loss.append(loss)\n",
    "    \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    After this point the most important work is improving the hyperparameters and improving the input data (feature engineering) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
